
<!DOCTYPE html>
<html lang="en">
<head>
    <title>Cap3D</title>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/css/bootstrap.min.css" integrity="sha384-rwoIResjU2yc3z8GV/NPeZWAv56rSmLldC3R/AZzGRnGxQQKnKkoFVhFQhNUwEyJ" crossorigin="anonymous">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="shortcut icon" href="static/img/favicon.ico">

    <!--[if lt IE 9]>
      <script src="http://cdn.bootcss.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="http://cdn.bootcss.com/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->
    <style type="text/css">
body{font-family:"Open Sans",Segoe,"Segoe UI","Lucida Sans Unicode","Lucida Grande","Avenir","Seravek","Ubuntu","DejaVu Sans","Trebuchet MS",Verdana,Arial,sans-serif}
#header{background:#00274C;opacity:0.95;margin:0 auto;padding:50px 0 0;text-align:center;cursor:default;text-align:center}
#header-container{margin:0 auto;padding: 0 2em;max-width:1000px}

#header-container .col-logo{text-align:left}
#header-container .logo{position:relative;z-index:100;height:50px;margin-top:20px;margin-right:10px}

#header-container .col-info{text-align:left;margin-bottom:20px}
#header-container #title{margin:.525em 0 1.525em;font-size:1.6em;font-weight:800;text-align:center}

.paper-info{color:#8a8989;display:inline-block;margin:0 auto;text-align:left}
.paper-info-margin{margin-bottom:20px}
.paper-info p, .paper-info h3{line-height:1.6em; margin-bottom: 0em}
.paper-info .title{font-size:16px;color:#FFCB05;font-weight:600}
.paper-info .authors, .paper-info .authors a{color:#989C97}
.paper-info .authors_bottom, .paper-info .authors_bottom a{color:#567EAE}
.paper-info .email{color:#666; font-size:14px}
.paper-info .tag{margin:auto auto;padding:0;list-style:none;text-align:left}
.paper-info .tag li{display:inline-block;margin:auto;padding:0 3px 0 0;line-height:10px;color:#567EAE} a{color:#567EAE}
.paper-info .conference, .paper-info .conference a{color:#FFCB05;font-weight:600}
.paper-info .top a{color:#989C97}

#wave-canvas{color:#8a8989;display:block;margin:-80px 0 0;width:100%;height:150px}

#content{padding-top:0px;text-align:left;}
#content-container{margin:0 auto;padding: 0 2em;max-width:1000px;}

#content-container .header{color:#00274C;background:#FFCB05;padding:15px 30px 5px;border-bottom:3px solid #dddddd}
#content-container .header .indicator{color:#00274C;margin-right:12px}

#content-container .content{background:#ffffff;padding:15px 5px 15px 30px}
#content-container .content .caption{color:#989C97}
#content-container .content .bib{color:#989C97;font-size:14px;}
#content-container .content .bib pre{margin:0;padding:0;}
#content-container .content .href{style="color:#567EAE"}



#footer{padding:2em 0 0.5em;margin:30px 0 0;background:#ffffff;opacity:0.95;font-size:14px;line-height:12px;text-align:center;color:#989C97}
.highlight, .highlight a{color:#BB2222;font-weight:600}
    </style>
</head>
<body>
<div id="main">
    <div id="header">
        <div id="header-container" class="container">
          <div class="row">
            <div class="col-md-12 col-xl-12 col-info">
                <h1 id="title"><p style="color:#FFCB05"> Cap3D </p></h1>
                <div class="paper-info">
                    <h3 class="title">Scalable 3D Captioning with Pretrained Models
</h3>
                    <p class="authors">
                    <a href="https://tiangeluo.github.io/">Tiange Luo*</a>, <a href="https://tiangeluo.github.io/">Chris Rockwell*</a>, <a href="https://web.eecs.umich.edu/~honglak/">Honglak Lee&#8224;</a>, <a href="https://web.eecs.umich.edu/~justincj/">Justin Johnson&#8224;</a> (*: equal contribution, &#8224;: equal advising)
                    </p>
                    <ul class="tag">
                        <li class="conference"><a href="https://arxiv.org/abs//2306.07279">arXiv 2023</a> </li>
                        <li class="top"><a href="https://arxiv.org/pdf/2306.07279.pdf">Paper</a> |</li>
                        <li class="top"><a href="https://github.com/crockwell/Cap3D">Code</a> |</li>
                        <li class="top"><a href="https://huggingface.co/datasets/tiange/Cap3D">Dataset</a> |</li>
                        <li class="top"><a href="https://tiangeluo.github.io/projectpages/bibs/cap3d.txt">BibTeX</a> </li>
                    </ul>
                </div>
            </div>
          </div>

        </div>
        <canvas id="wave-canvas"></canvas>
    </div>
    <div id="content">
        <div id="content-container" class="container">
            <div class="header"><h4><span class="indicator">=</span>Resources</h4></div>
            <div class="content">
                <ul>
                    <li>Data can be found in <a href="https://huggingface.co/datasets/tiange/Cap3D">[Huggingface]</a>, including descriptive captions for 3D objects in Objaverse and ABO, along with Objaverse's point clouds, rendered images, and Shap-E latent codes.</li>
                    <li>Our code for rendering, captioning, and finetuning text-to-3D models are released in <a href="https://github.com/crockwell/Cap3D">[Github]</a></li>
                    <li>Some fine-trained model checkpoints can be found in <a href="https://huggingface.co/datasets/tiange/Cap3D">[Huggingface]</a></a>.</li>
                    <li>More captioning examples can be found in <a href="https://tiangeluo.github.io/projectpages/imgs/Cap3D/000.html">[Link]</a></a>.</li>
                    <!--
                    <li>Training and inference codes  <a href="https://github.com/tiangeluo/">[PyTorch (Official)]</a>.</li>
                    <li>Used 3D data with part annotations can be downloaded in <a href="https://drive.google.com/">[Google Drive (.zip)]</a>.</li>
                    <li>Pre-trained models are provided in <a href="https://github.com/tiangeluo/">[Github repo]</a>.</li>
                    -->
                </ul>
            </div>
        </div>
        <div id="content-container" class="container">
            <div class="header"><h4><span class="indicator">=</span>Overview</h4></div>
            <div class="content">
            <style>
                .add-space {
                    margin-bottom: 20px;
                }
            </style>
            <p>
                Cap3D provides detailed descriptions of 3D objects by leveraging pretrained models in captioning, alignment, and LLM to consolidate multi-view information. 
            </p>

            <p>
                <center>
                <img width="100%" src="https://tiangeluo.github.io/projectpages/imgs/Cap3D/teaser.png">
                </center>
                </p>
                <p class="caption">
                <center>
                Figure 1:  Example captioning results by Cap3D.
                </center>
            </p>

            <p class="add-space">
                <center>
                <img width="100%" src="https://tiangeluo.github.io/projectpages/imgs/Cap3D/compare.png">
                </center>
                </p>
                <p class="caption">
                <center>
                Table 1:  Human evaluations revealed Cap3D surpasses crowdsourced annotation in efficiency, cost, and speed.
                </center>
                </p>
            <p>

            <p>
                Our proposed method, Cap3D, employs a four-step process. First, we render a set of 2D views for each 3D object. Next, we apply image captioning to achieve preliminary descriptions. As these captions may contain inaccuracies, an image-text alignment model, CLIP, is introduced in the third step to rectify errors. Finally, an LLM is employed to unify captions from various perspectives, creating a comprehensive caption. This process is shown in Figure 2 and detailed below.
            </p>
            <p>
                <center>
                <img width="100%" src="https://tiangeluo.github.io/projectpages/imgs/Cap3D/method.png">
                </center>
                </p>
                <p class="caption">
                <center>
                Figure 2:  Overview of Cap3D pipeline.
                </center>
            </p>
            </div>
        </div>
        <!----
        <div id="content-container" class="container">
            <div class="header"><h4><span class="indicator">=</span>Results</h4></div>
            <div class="content">
            <p>
            <center>
            <img width="80%" src="https://tiangeluo.github.io/projectpages/imgs/nsc/text2shape_v7.png"/>
            </center>
            </p>
            <p class="caption">
            <center>
            Figure 3: Text2PointCloud. Neural Shape Compiler can generate corresponding point clouds to the text prompt with structural details, while the baselines generate inaccurate structures or fewer alignments with the text prompt.
            </center>
            </p>

            <p>
            <center>
            <img width="80%" src="https://tiangeluo.github.io/projectpages/imgs/nsc/text2shape_pic2.png"/>
            </center>
            </p>
            <p class="caption">
            <center>
            Figure 4: Text2PointCloud more results.
            </center>
            </p>

            <p>
                <center>
                <img width="80%" src="https://tiangeluo.github.io/projectpages/imgs/nsc/shapecaptioning_v4.png"/>
                </center>
                </p>
                <p class="caption">
                <center>
                Figure 6: PointCloud2Text. One description per sentence. The shown shapes are from test sets, and Neural Shape Compiler tell their structures well.
                </center>
            </p>

            <p>
                <center>
                <img width="80%" src="https://tiangeluo.github.io/projectpages/imgs/nsc/shapeprogram_cmp_v5.png"/>
                </center>
                </p>
                <p class="caption">
                <center>
                Figure 6: PointCloud2Program. Neural Shape Compiler can well infer programs for the shown shapes of unseen categories and reconstruct shapes in a creative manner (e.g., represent the semicircle leg with bars).
                </center>
            </p>
        -->
	    <!---------
            <p>
            <center>
            <img width="80%" src="https://tiangeluo.github.io/projectpages/imgs/ltg/result_2.png"/>
            </center>
            </p>
            <p class="caption">
            <center>
            Figure 4: Quantitative results.
            </center>
            </p>
	    ------------->

            </div>
        </div>


        <div id="content-container" class="container">
            <div class="header"><h4><span class="indicator">=</span>Related Publication</h4></div>
            <div class="content">
                <div class="paper-info", style="margin-bottom: 15px;">
                    <h3 class="title">Objaverse: A Universe of Annotated 3D Objects</h3>
                    <p class="authors_bottom">
                        Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, Ali Farhadi
                    </p>
                    <ul class="tag">
                        <li class="conference"><a href="https://cvpr2023.thecvf.com/">CVPR 2023</a></a></li>
                        <li class="body"><a href="https://objaverse.allenai.org/">Project Page</a> </li>
                    </ul>
                </div>

                <div class="paper-info", style="margin-bottom: 15px;">
                    <h3 class="title">ABO: Dataset and Benchmarks for Real-World 3D Object Understanding</h3>
                    <p class="authors_bottom">
                        Jasmine Collins, Shubham Goel, Kenan Deng, Achleshwar Luthra, Leon Xu, Erhan Gundogdu, Xi Zhang, Tomas F. Yago Vicente, Thomas Dideriksen, Himanshu Arora, Matthieu Guillaumin, Jitendra Malik
                    </p>
                    <ul class="tag">
                        <li class="conference"><a href="https://cvpr2022.thecvf.com/">CVPR 2022</a></a></li>
                        <li class="body"><a href="https://amazon-berkeley-objects.s3.amazonaws.com/index.html">Project Page</a> </li>
                    </ul>
                </div>
                <div class="paper-info", style="margin-bottom: 15px;">
                    <h3 class="title">GPT-4 Technical Report</h3>
                    <p class="authors_bottom">
                        OpenAI
                    </p>
                    <ul class="tag">
                        <li class="conference"><a href="https://arxiv.org/abs/2303.08774">arXiv 2023</a></a></li>
                        <li class="body"><a href="https://arxiv.org/pdf/2303.08774.pdf">Page</a> </li>
                    </ul>
                </div>
                <div class="paper-info", style="margin-bottom: 15px;">
                    <h3 class="title">BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models</h3>
                    <p class="authors_bottom">
                        Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi
                    </p>
                    <ul class="tag">
                        <li class="conference"><a href="https://arxiv.org/abs/2301.12597">arXiv 2023</a></a></li>
                        <li class="body"><a href="https://arxiv.org/pdf/2301.12597.pdf">Page</a> </li>
                    </ul>
                </div>
            </div>
        </div>
    </div>
    <div id="footer">
        <p>Thank this <a href="http://nscl.csail.mit.edu/">template</a>. <a href="https://accessibility.umich.edu/">Accessibility</a>. </p>
    </div>
    <!----------
    <div id="footer">
        <p>Thank this <a href="http://nscl.csail.mit.edu/">template</a> </p>
    </div>
    ---------->
</div>

<!-- jQuery first, then Tether, then Bootstrap JS. -->
<script src="https://code.jquery.com/jquery-3.1.1.min.js" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/tether/1.4.0/js/tether.min.js"
        integrity="sha384-DztdAPBWPRXSA/3eYEEUWrWCy7G5KFbe8fFjk5JAIxUYHKkDx6Qin1DkWx51bBrb" crossorigin="anonymous"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/js/bootstrap.min.js"
        integrity="sha384-vBWWzlZJ8ea9aCX4pEW3rVHjgjt7zpkNpZk+02D9phzyeVkE+jo0ieGizqPLForn" crossorigin="anonymous"></script>
<script type="text/javascript" src="static/js/jquery.color.min.js"></script>
<script type="text/javascript" src="static/js/wave.js"></script>
<script type="text/javascript">
$(function() {
    targetColor = $("#title").css("color")
    animatedLink = function(speed) {
        $(".link-li").hover(function() {
            $(this).find('.icon').animate({
                color: targetColor,
                borderColor: targetColor
            }, speed);
            $(this).find('.caption').animate({
                color: '#798350'
            })
        }, function() {
            $(this).find('.icon').animate({
                borderColor: '#cccccc',
                color: '#cccccc'
            }, speed);
            $(this).find('.caption').animate({
                color: '#cccccc'
            })
        })
    };
    // fullBg();
    animatedLink(400)
});
</script>
</body>
</html>
